From 155421f0d807badf4cc0f364c7b287c2b331f6f0 Mon Sep 17 00:00:00 2001
From: Vitaliy Kuznetsov <ahil52_25@mail.ru>
Date: Tue, 2 Mar 2021 19:00:45 +0300
Subject: [PATCH] Add vhost-user-nvme

---
 hw/block/meson.build       |    1 +
 hw/block/nvme-ns.h         |    1 +
 hw/block/nvme.h            |   36 ++
 hw/block/vhost_user_nvme.c | 1077 ++++++++++++++++++++++++++++++++++++
 hw/block/vhost_user_nvme.h |   35 ++
 hw/virtio/vhost-user.c     |  201 +++++++
 hw/virtio/vhost.c          |  112 ++++
 include/block/nvme.h       |    1 +
 8 files changed, 1464 insertions(+)
 create mode 100644 hw/block/vhost_user_nvme.c
 create mode 100644 hw/block/vhost_user_nvme.h

diff --git a/hw/block/meson.build b/hw/block/meson.build
index 602ca6c8541..0e2e4aab4b0 100644
--- a/hw/block/meson.build
+++ b/hw/block/meson.build
@@ -14,6 +14,7 @@ softmmu_ss.add(when: 'CONFIG_SWIM', if_true: files('swim.c'))
 softmmu_ss.add(when: 'CONFIG_XEN', if_true: files('xen-block.c'))
 softmmu_ss.add(when: 'CONFIG_SH4', if_true: files('tc58128.c'))
 softmmu_ss.add(when: 'CONFIG_NVME_PCI', if_true: files('nvme.c', 'nvme-ns.c'))
+softmmu_ss.add(when: 'CONFIG_LINUX', if_true: files('vhost_user_nvme.c'))

 specific_ss.add(when: 'CONFIG_VIRTIO_BLK', if_true: files('virtio-blk.c'))
 specific_ss.add(when: 'CONFIG_VHOST_USER_BLK', if_true: files('vhost-user-blk.c'))
diff --git a/hw/block/nvme-ns.h b/hw/block/nvme-ns.h
index 83734f4606e..f15aee41ffb 100644
--- a/hw/block/nvme-ns.h
+++ b/hw/block/nvme-ns.h
@@ -14,6 +14,7 @@

 #ifndef NVME_NS_H
 #define NVME_NS_H
+#include "qemu/uuid.h"

 #define TYPE_NVME_NS "nvme-ns"
 #define NVME_NS(obj) \
diff --git a/hw/block/nvme.h b/hw/block/nvme.h
index e080a2318a5..d0891522296 100644
--- a/hw/block/nvme.h
+++ b/hw/block/nvme.h
@@ -3,6 +3,11 @@

 #include "block/nvme.h"
 #include "nvme-ns.h"
+#include "hw/virtio/vhost.h"
+#include "hw/virtio/vhost-user.h"
+#include "sysemu/hostmem.h"
+#include "chardev/char-fe.h"
+#include "hw/pci/pci.h"

 #define NVME_MAX_NAMESPACES 256

@@ -36,6 +41,18 @@ typedef struct NvmeRequest {
     QTAILQ_ENTRY(NvmeRequest)entry;
 } NvmeRequest;

+typedef struct NvmeStatus {
+    uint16_t p:1;     /* phase tag */
+    uint16_t sc:8;    /* status code */
+    uint16_t sct:3;   /* status code type */
+    uint16_t rsvd2:2;
+    uint16_t m:1;     /* more */
+    uint16_t dnr:1;   /* do not retry */
+} NvmeStatus;
+
+#define nvme_cpl_is_error(status) \
+        (((status & 0x01fe) != 0) || ((status & 0x0e00) != 0))
+
 static inline const char *nvme_adm_opc_str(uint8_t opc)
 {
     switch (opc) {
@@ -89,6 +106,10 @@ typedef struct NvmeCQueue {
     uint32_t    vector;
     uint32_t    size;
     uint64_t    dma_addr;
+
+    int32_t     virq;
+    EventNotifier guest_notifier;
+
     QEMUTimer   *timer;
     QTAILQ_HEAD(, NvmeSQueue) sq_list;
     QTAILQ_HEAD(, NvmeRequest) req_list;
@@ -105,6 +126,10 @@ typedef struct NvmeBus {
 #define NVME(obj) \
         OBJECT_CHECK(NvmeCtrl, (obj), TYPE_NVME)

+#define TYPE_VHOST_NVME "vhost-user-nvme"
+#define NVME_VHOST(obj) \
+        OBJECT_CHECK(NvmeCtrl, (obj), TYPE_VHOST_NVME)
+
 typedef struct NvmeFeatureVal {
     struct {
         uint16_t temp_thresh_hi;
@@ -123,6 +148,17 @@ typedef struct NvmeCtrl {
     NvmeBus      bus;
     BlockConf    conf;

+    MemoryRegion      *shadow_mr;
+    volatile uint32_t *shadow_db;
+    HostMemoryBackend *barmem;
+    int32_t    bootindex;
+    CharBackend chardev;
+    VhostUserState vhost_user;
+    struct vhost_dev dev;
+    uint32_t    num_io_queues;
+    bool        dataplane_started;
+    bool        vector_poll_started;
+
     bool        qs_created;
     uint32_t    page_size;
     uint16_t    page_bits;
diff --git a/hw/block/vhost_user_nvme.c b/hw/block/vhost_user_nvme.c
new file mode 100644
index 00000000000..042d1b7ab27
--- /dev/null
+++ b/hw/block/vhost_user_nvme.c
@@ -0,0 +1,1077 @@
+/*
+ * QEMU NVM Express Controller
+ *
+ * Copyright (c) 2019, Intel Corporation
+ *
+ * Author:
+ * Changpeng Liu <changpeng.liu@intel.com>
+ *
+ * This work was largely based on QEMU NVMe driver implementation by:
+ * Keith Busch <keith.busch@intel.com>
+ *
+ * This code is licensed under the GNU GPL v2 or later.
+ */
+
+/**
+ * Reference Specs: http://www.nvmexpress.org, 1.2, 1.1, 1.0e
+ *
+ *  http://www.nvmexpress.org/resources/
+ */
+
+#include "qemu/osdep.h"
+#include "hw/block/block.h"
+#include "hw/hw.h"
+#include "sysemu/kvm.h"
+#include "hw/pci/msi.h"
+#include "hw/pci/msix.h"
+#include "hw/pci/pci.h"
+#include "hw/qdev-properties.h"
+#include "sysemu/sysemu.h"
+#include "qapi/error.h"
+#include "qemu/error-report.h"
+#include "qapi/visitor.h"
+#include "nvme.h"
+#include "nvme-ns.h"
+#include "vhost_user_nvme.h"
+
+static int vhost_user_nvme_vector_unmask(PCIDevice *dev, unsigned vector,
+                                         MSIMessage msg)
+{
+    NvmeCtrl *n = container_of(dev, NvmeCtrl, parent_obj);
+    NvmeCQueue *cq;
+    EventNotifier *e;
+    uint32_t qid;
+    int ret;
+
+    for (qid = 1; qid <= n->num_io_queues; qid++) {
+        cq = n->cq[qid];
+        if (!cq) {
+            continue;
+        }
+
+        if (cq->vector == vector) {
+            e = &cq->guest_notifier;
+            ret = kvm_irqchip_update_msi_route(kvm_state, cq->virq, msg, dev);
+            if (ret < 0) {
+                error_report("msi irq update vector %u failed", vector);
+                return ret;
+            }
+            kvm_irqchip_commit_routes(kvm_state);
+            ret = kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, e,
+                                                     NULL, cq->virq);
+            if (ret < 0) {
+                error_report("msi add irqfd gsi vector %u failed, ret %d",
+                             vector, ret);
+                return ret;
+            }
+            return 0;
+        }
+    }
+
+    return 0;
+}
+
+static void vhost_user_nvme_vector_mask(PCIDevice *dev, unsigned vector)
+{
+    NvmeCtrl *n = container_of(dev, NvmeCtrl, parent_obj);
+    NvmeCQueue *cq;
+    EventNotifier *e;
+    uint32_t qid;
+    int ret;
+
+    for (qid = 1; qid <= n->num_io_queues; qid++) {
+        cq = n->cq[qid];
+        if (!cq) {
+            continue;
+        }
+
+        if (cq->vector == vector) {
+            e = &cq->guest_notifier;
+            ret = kvm_irqchip_remove_irqfd_notifier_gsi(kvm_state, e, cq->virq);
+            if (ret != 0) {
+                error_report("remove_irqfd_notifier_gsi failed");
+            }
+            return;
+        }
+    }
+
+    return;
+}
+
+static void vhost_user_nvme_vector_poll(PCIDevice *dev,
+                                        unsigned int vector_start,
+                                        unsigned int vector_end)
+{
+    NvmeCtrl *n = container_of(dev, NvmeCtrl, parent_obj);
+    NvmeCQueue *cq;
+    EventNotifier *e;
+    uint32_t qid, vector;
+
+    for (qid = 1; qid <= n->num_io_queues; qid++) {
+        cq = n->cq[qid];
+        if (!cq) {
+            continue;
+        }
+
+        vector = cq->vector;
+        if (vector < vector_end && vector >= vector_start) {
+            e = &cq->guest_notifier;
+            if (!msix_is_masked(dev, vector)) {
+                continue;
+            }
+
+            if (event_notifier_test_and_clear(e)) {
+                msix_set_pending(dev, vector);
+            }
+        }
+    }
+}
+
+static bool nvme_nsid_valid(NvmeCtrl *n, uint32_t nsid)
+{
+    return nsid && (nsid == NVME_NSID_BROADCAST || nsid <= n->num_namespaces);
+}
+
+static int vhost_user_nvme_add_kvm_msi_virq(NvmeCtrl *n, NvmeCQueue *cq)
+{
+    int virq;
+    int vector_n;
+
+    if (!msix_enabled(&(n->parent_obj))) {
+        error_report("MSIX is mandatory for the device");
+        return -1;
+    }
+
+    if (event_notifier_init(&cq->guest_notifier, 0)) {
+        error_report("Initiated guest notifier failed");
+        return -1;
+    }
+    event_notifier_set_handler(&cq->guest_notifier, NULL);
+
+    vector_n = cq->vector;
+
+    virq = kvm_irqchip_add_msi_route(kvm_state, vector_n, &n->parent_obj);
+    if (virq < 0) {
+        error_report("Route MSIX vector to KVM failed");
+        event_notifier_cleanup(&cq->guest_notifier);
+        return -1;
+    }
+    cq->virq = virq;
+
+    return 0;
+}
+
+static void vhost_user_nvme_remove_kvm_msi_virq(NvmeCQueue *cq)
+{
+    kvm_irqchip_release_virq(kvm_state, cq->virq);
+    event_notifier_cleanup(&cq->guest_notifier);
+    cq->virq = -1;
+}
+
+static int nvme_check_sqid(NvmeCtrl *n, uint16_t sqid)
+{
+    if (sqid < n->num_io_queues + 1) {
+        return 0;
+    }
+
+    return 1;
+}
+
+static int nvme_check_cqid(NvmeCtrl *n, uint16_t cqid)
+{
+    if (cqid < n->num_io_queues + 1) {
+        return 0;
+    }
+
+    return 1;
+}
+
+static void nvme_inc_cq_tail(NvmeCQueue *cq)
+{
+    cq->tail++;
+    if (cq->tail >= cq->size) {
+        cq->tail = 0;
+        cq->phase = !cq->phase;
+    }
+}
+
+static void nvme_inc_sq_head(NvmeSQueue *sq)
+{
+    sq->head = (sq->head + 1) % sq->size;
+}
+
+static uint8_t nvme_sq_empty(NvmeSQueue *sq)
+{
+    return sq->head == sq->tail;
+}
+
+static void nvme_isr_notify(NvmeCtrl *n, NvmeCQueue *cq)
+{
+    if (cq->irq_enabled) {
+        if (msix_enabled(&(n->parent_obj))) {
+            msix_notify(&(n->parent_obj), cq->vector);
+        } else {
+            pci_irq_pulse(&n->parent_obj);
+        }
+    }
+}
+
+static void nvme_free_sq(NvmeSQueue *sq, NvmeCtrl *n)
+{
+    if (sq->sqid) {
+        n->sq[sq->sqid] = NULL;
+        g_free(sq);
+    }
+}
+
+static uint16_t nvme_del_sq(NvmeCtrl *n, NvmeCmd *cmd)
+{
+    NvmeDeleteQ *c = (NvmeDeleteQ *)cmd;
+    NvmeSQueue *sq;
+    NvmeCqe cqe;
+    uint16_t qid = le16_to_cpu(c->qid);
+    int ret;
+
+    if (!qid || nvme_check_sqid(n, qid)) {
+        error_report("nvme_del_sq: invalid qid %u", qid);
+        return NVME_INVALID_QID | NVME_DNR;
+    }
+
+    sq = n->sq[qid];
+
+    ret = vhost_user_nvme_admin_cmd_raw(&n->dev, cmd, &cqe, sizeof(cqe));
+    if (ret < 0) {
+        error_report("nvme_del_sq: delete sq failed");
+        return -1;
+    }
+
+    nvme_free_sq(sq, n);
+    return NVME_SUCCESS;
+}
+
+static void nvme_init_sq(NvmeSQueue *sq, NvmeCtrl *n, uint64_t dma_addr,
+    uint16_t sqid, uint16_t cqid, uint16_t size)
+{
+    sq->ctrl = n;
+    sq->dma_addr = dma_addr;
+    sq->sqid = sqid;
+    sq->size = size;
+    sq->cqid = cqid;
+    sq->head = sq->tail = 0;
+
+    n->sq[sqid] = sq;
+}
+
+static uint16_t nvme_create_sq(NvmeCtrl *n, NvmeCmd *cmd)
+{
+    NvmeSQueue *sq;
+    int ret;
+    NvmeCqe cqe;
+    NvmeCreateSq *c = (NvmeCreateSq *)cmd;
+
+    uint16_t cqid = le16_to_cpu(c->cqid);
+    uint16_t sqid = le16_to_cpu(c->sqid);
+    uint16_t qsize = le16_to_cpu(c->qsize);
+    uint16_t qflags = le16_to_cpu(c->sq_flags);
+    uint64_t prp1 = le64_to_cpu(c->prp1);
+
+    if (!cqid) {
+        error_report("nvme_create_sq: invalid cqid %u", cqid);
+        return NVME_INVALID_CQID | NVME_DNR;
+    }
+    if (!sqid || nvme_check_sqid(n, sqid)) {
+        error_report("nvme_create_sq: invalid sqid");
+        return NVME_INVALID_QID | NVME_DNR;
+    }
+    if (!qsize || qsize > NVME_CAP_MQES(n->bar.cap)) {
+        error_report("nvme_create_sq: invalid qsize");
+        return NVME_MAX_QSIZE_EXCEEDED | NVME_DNR;
+    }
+    if (!prp1 || prp1 & (n->page_size - 1)) {
+        error_report("nvme_create_sq: invalid prp1");
+        return NVME_INVALID_FIELD | NVME_DNR;
+    }
+    if (!(NVME_SQ_FLAGS_PC(qflags))) {
+        error_report("nvme_create_sq: invalid flags");
+        return NVME_INVALID_FIELD | NVME_DNR;
+    }
+
+    /* BIOS also create IO queue pair for same queue ID */
+    if (n->sq[sqid] != NULL) {
+        nvme_free_sq(n->sq[sqid], n);
+    }
+
+    sq = g_malloc0(sizeof(*sq));
+    assert(sq != NULL);
+    nvme_init_sq(sq, n, prp1, sqid, cqid, qsize + 1);
+    ret = vhost_user_nvme_admin_cmd_raw(&n->dev, cmd, &cqe, sizeof(cqe));
+    if (ret < 0) {
+        error_report("nvme_create_sq: create sq failed");
+        return -1;
+    }
+    return NVME_SUCCESS;
+}
+
+static void nvme_free_cq(NvmeCQueue *cq, NvmeCtrl *n)
+{
+    msix_vector_unuse(&n->parent_obj, cq->vector);
+    if (cq->cqid) {
+        n->cq[cq->cqid] = NULL;
+        g_free(cq);
+    }
+}
+
+static uint16_t nvme_del_cq(NvmeCtrl *n, NvmeCmd *cmd)
+{
+    NvmeDeleteQ *c = (NvmeDeleteQ *)cmd;
+    NvmeCqe cqe;
+    NvmeCQueue *cq;
+    uint16_t qid = le16_to_cpu(c->qid);
+    int ret;
+
+    if (!qid || nvme_check_cqid(n, qid)) {
+        error_report("nvme_del_cq: invalid qid %u", qid);
+        return NVME_INVALID_CQID | NVME_DNR;
+    }
+
+    ret = vhost_user_nvme_admin_cmd_raw(&n->dev, cmd, &cqe, sizeof(cqe));
+    if (ret < 0) {
+        error_report("nvme_del_cq: delete cq failed");
+        return -1;
+    }
+
+    cq = n->cq[qid];
+
+    nvme_free_cq(cq, n);
+    return NVME_SUCCESS;
+}
+
+static void nvme_init_cq(NvmeCQueue *cq, NvmeCtrl *n, uint64_t dma_addr,
+    uint16_t cqid, uint16_t vector, uint16_t size, uint16_t irq_enabled)
+{
+    cq->ctrl = n;
+    cq->cqid = cqid;
+    cq->size = size;
+    cq->dma_addr = dma_addr;
+    cq->phase = 1;
+    cq->irq_enabled = irq_enabled;
+    cq->vector = vector;
+    cq->head = cq->tail = 0;
+    msix_vector_unuse(&n->parent_obj, cq->vector);
+    if (msix_vector_use(&n->parent_obj, cq->vector) < 0) {
+        error_report("nvme_init_cq: init cq vector failed");
+    }
+    n->cq[cqid] = cq;
+}
+
+static uint16_t nvme_create_cq(NvmeCtrl *n, NvmeCmd *cmd)
+{
+    int ret;
+    NvmeCQueue *cq;
+    NvmeCqe cqe;
+    NvmeCreateCq *c = (NvmeCreateCq *)cmd;
+    uint16_t cqid = le16_to_cpu(c->cqid);
+    uint16_t vector = le16_to_cpu(c->irq_vector);
+    uint16_t qsize = le16_to_cpu(c->qsize);
+    uint16_t qflags = le16_to_cpu(c->cq_flags);
+    uint64_t prp1 = le64_to_cpu(c->prp1);
+
+    if (!cqid || nvme_check_cqid(n, cqid)) {
+        error_report("nvme_create_cq: invalid cqid");
+        return NVME_INVALID_CQID | NVME_DNR;
+    }
+    if (!qsize || qsize > NVME_CAP_MQES(n->bar.cap)) {
+        error_report("nvme_create_cq: invalid qsize, qsize %u", qsize);
+        return NVME_MAX_QSIZE_EXCEEDED | NVME_DNR;
+    }
+    if (!prp1) {
+        error_report("nvme_create_cq: invalid prp1");
+        return NVME_INVALID_FIELD | NVME_DNR;
+    }
+    if (vector > n->num_io_queues + 1) {
+        error_report("nvme_create_cq: invalid irq vector");
+        return NVME_INVALID_IRQ_VECTOR | NVME_DNR;
+    }
+    if (!(NVME_CQ_FLAGS_PC(qflags))) {
+        error_report("nvme_create_cq: invalid flags");
+        return NVME_INVALID_FIELD | NVME_DNR;
+    }
+
+    /* BIOS also create IO queue pair for same queue ID */
+    if (n->cq[cqid] != NULL) {
+        nvme_free_cq(n->cq[cqid], n);
+    }
+
+    cq = g_malloc0(sizeof(*cq));
+    assert(cq != NULL);
+    nvme_init_cq(cq, n, prp1, cqid, vector, qsize + 1,
+                 NVME_CQ_FLAGS_IEN(qflags));
+    ret = vhost_user_nvme_admin_cmd_raw(&n->dev, cmd, &cqe, sizeof(cqe));
+    if (ret < 0) {
+        error_report("nvme_create_cq: create cq failed");
+        return -1;
+    }
+
+    if (cq->irq_enabled) {
+        ret = vhost_user_nvme_add_kvm_msi_virq(n, cq);
+        if (ret < 0) {
+            error_report("vhost-user-nvme: add kvm msix virq failed");
+            return -1;
+        }
+        ret = vhost_dev_nvme_set_guest_notifier(&n->dev,
+                                                &cq->guest_notifier,
+                                                cq->cqid);
+        if (ret < 0) {
+            error_report("vhost-user-nvme: set guest notifier failed");
+            return -1;
+        }
+    }
+
+    if (cq->irq_enabled && !n->vector_poll_started) {
+        n->vector_poll_started = true;
+        if (msix_set_vector_notifiers(&n->parent_obj,
+                                      vhost_user_nvme_vector_unmask,
+                                      vhost_user_nvme_vector_mask,
+                                      vhost_user_nvme_vector_poll)) {
+            error_report("vhost-user-nvme: msix_set_vector_notifiers failed");
+            return -1;
+        }
+    }
+
+    return NVME_SUCCESS;
+}
+
+static uint16_t nvme_identify_ctrl(NvmeCtrl *n, NvmeIdentify *c)
+{
+    uint64_t prp1 = le64_to_cpu(c->prp1);
+
+    /* Only PRP1 used */
+    pci_dma_write(&n->parent_obj, prp1, (void *)&n->id_ctrl,
+                 sizeof(n->id_ctrl));
+    return NVME_SUCCESS;
+}
+
+static uint16_t nvme_identify_ns(NvmeCtrl *n, NvmeIdentify *c)
+{
+    NvmeNamespace *ns;
+    uint32_t nsid = le32_to_cpu(c->nsid);
+    uint64_t prp1 = le64_to_cpu(c->prp1);
+
+    if (!nvme_nsid_valid(n, nsid) || nsid == NVME_NSID_BROADCAST) {
+        return NVME_INVALID_NSID | NVME_DNR;
+    }
+
+    /* Only PRP1 used */
+    ns = nvme_ns(n, nsid);
+    pci_dma_write(&n->parent_obj, prp1, (void *)ns, sizeof(*ns));
+    return NVME_SUCCESS;
+}
+
+static uint16_t nvme_identify(NvmeCtrl *n, NvmeCmd *cmd)
+{
+    NvmeIdentify *c = (NvmeIdentify *)cmd;
+
+    switch (le32_to_cpu(c->cns)) {
+    case 0x00:
+        return nvme_identify_ns(n, c);
+    case 0x01:
+        return nvme_identify_ctrl(n, c);
+    default:
+        return NVME_INVALID_FIELD | NVME_DNR;
+    }
+}
+
+static uint16_t nvme_get_feature(NvmeCtrl *n, NvmeCmd *cmd, NvmeCqe *cqe)
+{
+    uint32_t dw10 = le32_to_cpu(cmd->cdw10);
+    int ret;
+
+    switch (dw10 & 0xff) {
+    case NVME_VOLATILE_WRITE_CACHE:
+        cqe->result = 0;
+        break;
+    case NVME_NUMBER_OF_QUEUES:
+        ret = vhost_user_nvme_admin_cmd_raw(&n->dev, cmd, cqe, sizeof(*cqe));
+        if (ret < 0) {
+            return NVME_INVALID_FIELD | NVME_DNR;
+        }
+        /* 0 based value for number of IO queues */
+        if (n->num_io_queues > (cqe->result & 0xffffu) + 1) {
+            info_report("Adjust number of IO queues from %u to %u",
+                    n->num_io_queues, (cqe->result & 0xffffu) + 1);
+                    n->num_io_queues = (cqe->result & 0xffffu) + 1;
+        }
+        break;
+    default:
+        return NVME_INVALID_FIELD | NVME_DNR;
+    }
+
+    return NVME_SUCCESS;
+}
+
+static uint16_t nvme_set_feature(NvmeCtrl *n, NvmeCmd *cmd, NvmeCqe *cqe)
+{
+    uint32_t dw10 = le32_to_cpu(cmd->cdw10);
+    int ret;
+
+    switch (dw10 & 0xff) {
+    case NVME_NUMBER_OF_QUEUES:
+        ret = vhost_user_nvme_admin_cmd_raw(&n->dev, cmd, cqe, sizeof(*cqe));
+        if (ret < 0) {
+            return NVME_INVALID_FIELD | NVME_DNR;
+        }
+        /* 0 based value for number of IO queues */
+        if (n->num_io_queues > (cqe->result & 0xffffu) + 1) {
+            info_report("Adjust number of IO queues from %u to %u",
+                    n->num_io_queues, (cqe->result & 0xffffu) + 1);
+                    n->num_io_queues = (cqe->result & 0xffffu) + 1;
+        }
+        break;
+    default:
+        return NVME_INVALID_FIELD | NVME_DNR;
+    }
+    return NVME_SUCCESS;
+}
+
+static void nvme_clear_guest_notifier(NvmeCtrl *n)
+{
+    NvmeCQueue *cq;
+    uint32_t qid;
+
+    for (qid = 1; qid <= n->num_io_queues; qid++) {
+        cq = n->cq[qid];
+        if (!cq) {
+            break;
+        }
+
+        if (cq->irq_enabled) {
+            vhost_user_nvme_remove_kvm_msi_virq(cq);
+        }
+    }
+
+    if (n->vector_poll_started) {
+        msix_unset_vector_notifiers(&n->parent_obj);
+        n->vector_poll_started = false;
+    }
+}
+
+static uint16_t nvme_doorbell_buffer_config(NvmeCtrl *n, NvmeCmd *cmd)
+{
+    int ret;
+    NvmeCmd cqe;
+
+    ret = vhost_user_nvme_admin_cmd_raw(&n->dev, cmd, &cqe, sizeof(cqe));
+    if (ret < 0) {
+        error_report("nvme_doorbell_buffer_config: set failed");
+        return NVME_INVALID_FIELD | NVME_DNR;
+    }
+
+    n->dataplane_started = true;
+
+    return NVME_SUCCESS;
+}
+
+static uint16_t nvme_abort_cmd(NvmeCtrl *n, NvmeCmd *cmd)
+{
+    int ret;
+    NvmeCmd cqe;
+
+    ret = vhost_user_nvme_admin_cmd_raw(&n->dev, cmd, &cqe, sizeof(cqe));
+    if (ret < 0) {
+        error_report("nvme_abort_cmd: set failed");
+        return NVME_INVALID_FIELD | NVME_DNR;
+    }
+
+    return NVME_SUCCESS;
+}
+
+static const char *nvme_admin_str[256] = {
+    [NVME_ADM_CMD_IDENTIFY] = "NVME_ADM_CMD_IDENTIFY",
+    [NVME_ADM_CMD_CREATE_CQ] = "NVME_ADM_CMD_CREATE_CQ",
+    [NVME_ADM_CMD_GET_LOG_PAGE] = "NVME_ADM_CMD_GET_LOG_PAGE",
+    [NVME_ADM_CMD_CREATE_SQ] = "NVME_ADM_CMD_CREATE_SQ",
+    [NVME_ADM_CMD_DELETE_CQ] = "NVME_ADM_CMD_DELETE_CQ",
+    [NVME_ADM_CMD_DELETE_SQ] = "NVME_ADM_CMD_DELETE_SQ",
+    [NVME_ADM_CMD_SET_FEATURES] = "NVME_ADM_CMD_SET_FEATURES",
+    [NVME_ADM_CMD_GET_FEATURES] = "NVME_ADM_CMD_SET_FEATURES",
+    [NVME_ADM_CMD_ABORT] = "NVME_ADM_CMD_ABORT",
+    [NVME_ADM_CMD_SET_DB_MEMORY] = "NVME_ADM_CMD_SET_DB_MEMORY",
+};
+
+static uint16_t nvme_admin_cmd(NvmeCtrl *n, NvmeCmd *cmd, NvmeCqe *cqe)
+{
+    info_report("QEMU Processing %s", nvme_admin_str[cmd->opcode] ?
+            nvme_admin_str[cmd->opcode] : "Unsupported ADMIN Command");
+
+    switch (cmd->opcode) {
+    case NVME_ADM_CMD_DELETE_SQ:
+        return nvme_del_sq(n, cmd);
+    case NVME_ADM_CMD_CREATE_SQ:
+        return nvme_create_sq(n, cmd);
+    case NVME_ADM_CMD_DELETE_CQ:
+        return nvme_del_cq(n, cmd);
+    case NVME_ADM_CMD_CREATE_CQ:
+        return nvme_create_cq(n, cmd);
+    case NVME_ADM_CMD_IDENTIFY:
+        return nvme_identify(n, cmd);
+    case NVME_ADM_CMD_SET_FEATURES:
+        return nvme_set_feature(n, cmd, cqe);
+    case NVME_ADM_CMD_GET_FEATURES:
+        return nvme_get_feature(n, cmd, cqe);
+    case NVME_ADM_CMD_SET_DB_MEMORY:
+        return nvme_doorbell_buffer_config(n, cmd);
+    case NVME_ADM_CMD_ABORT:
+        return nvme_abort_cmd(n, cmd);
+    default:
+        return NVME_INVALID_OPCODE | NVME_DNR;
+    }
+}
+
+static int nvme_start_ctrl(NvmeCtrl *n)
+{
+    uint32_t page_bits = NVME_CC_MPS(n->bar.cc) + 12;
+    uint32_t page_size = 1 << page_bits;
+
+    info_report("QEMU Start NVMe Controller ...");
+    if (vhost_dev_nvme_start(&n->dev, NULL) < 0) {
+        error_report("nvme_start_ctrl: vhost device start failed");
+        return -1;
+    }
+
+    if (!n->bar.asq || !n->bar.acq ||
+            n->bar.asq & (page_size - 1) || n->bar.acq & (page_size - 1) ||
+            NVME_CC_MPS(n->bar.cc) < NVME_CAP_MPSMIN(n->bar.cap) ||
+            NVME_CC_MPS(n->bar.cc) > NVME_CAP_MPSMAX(n->bar.cap) ||
+            !NVME_AQA_ASQS(n->bar.aqa) || !NVME_AQA_ACQS(n->bar.aqa)) {
+        error_report("nvme_start_ctrl: invalid bar configurations");
+        return -1;
+    }
+
+    n->page_bits = page_bits;
+    n->page_size = page_size;
+    n->max_prp_ents = n->page_size / sizeof(uint64_t);
+    n->cqe_size = 1 << NVME_CC_IOCQES(n->bar.cc);
+    n->sqe_size = 1 << NVME_CC_IOSQES(n->bar.cc);
+    nvme_init_cq(&n->admin_cq, n, n->bar.acq, 0, 0,
+        NVME_AQA_ACQS(n->bar.aqa) + 1, 1);
+    nvme_init_sq(&n->admin_sq, n, n->bar.asq, 0, 0,
+        NVME_AQA_ASQS(n->bar.aqa) + 1);
+
+    return 0;
+}
+
+static int nvme_clear_ctrl(NvmeCtrl *n, bool shutdown)
+{
+    int i;
+
+    if (shutdown) {
+        info_report("QEMU Shutdown NVMe Controller ...");
+    } else {
+        info_report("QEMU Disable NVMe Controller ...");
+    }
+
+    if (vhost_dev_nvme_stop(&n->dev) < 0) {
+        error_report("nvme_clear_ctrl: vhost device stop failed");
+        return -1;
+    }
+
+    if (shutdown) {
+        nvme_clear_guest_notifier(n);
+    }
+
+    for (i = 0; i <= n->num_io_queues; i++) {
+        if (n->sq[i] != NULL) {
+            nvme_free_sq(n->sq[i], n);
+        }
+
+        if (n->cq[i] != NULL) {
+            nvme_free_cq(n->cq[i], n);
+        }
+    }
+
+    n->bar.cc = 0;
+    n->dataplane_started = false;
+    return 0;
+}
+
+static void nvme_write_bar(NvmeCtrl *n, hwaddr offset, uint64_t data,
+                           unsigned size)
+{
+    switch (offset) {
+    case 0xc:
+        n->bar.intms |= data & 0xffffffff;
+        n->bar.intmc = n->bar.intms;
+        break;
+    case 0x10:
+        n->bar.intms &= ~(data & 0xffffffff);
+        n->bar.intmc = n->bar.intms;
+        break;
+    case 0x14:
+        /* Windows first sends data, then sends enable bit */
+        if (!NVME_CC_EN(data) && !NVME_CC_EN(n->bar.cc) &&
+            !NVME_CC_SHN(data) && !NVME_CC_SHN(n->bar.cc))
+        {
+            n->bar.cc = data;
+        }
+
+        if (NVME_CC_EN(data) && !NVME_CC_EN(n->bar.cc)) {
+            n->bar.cc = data;
+            if (nvme_start_ctrl(n)) {
+                n->bar.csts = NVME_CSTS_FAILED;
+            } else {
+                n->bar.csts = NVME_CSTS_READY;
+            }
+        } else if (!NVME_CC_EN(data) && NVME_CC_EN(n->bar.cc)) {
+            nvme_clear_ctrl(n, false);
+            n->bar.csts &= ~NVME_CSTS_READY;
+        }
+        if (NVME_CC_SHN(data) && !(NVME_CC_SHN(n->bar.cc))) {
+                nvme_clear_ctrl(n, true);
+                n->bar.cc = data;
+                n->bar.csts |= NVME_CSTS_SHST_COMPLETE;
+        } else if (!NVME_CC_SHN(data) && NVME_CC_SHN(n->bar.cc)) {
+                n->bar.csts &= ~NVME_CSTS_SHST_COMPLETE;
+                n->bar.cc = data;
+        }
+        break;
+    case 0x24:
+        n->bar.aqa = data & 0xffffffff;
+        break;
+    case 0x28:
+        n->bar.asq = data;
+        break;
+    case 0x2c:
+        n->bar.asq |= data << 32;
+        break;
+    case 0x30:
+        n->bar.acq = data;
+        break;
+    case 0x34:
+        n->bar.acq |= data << 32;
+        break;
+    default:
+        break;
+    }
+}
+
+static uint64_t nvme_mmio_read(void *opaque, hwaddr addr, unsigned size)
+{
+    NvmeCtrl *n = (NvmeCtrl *)opaque;
+    uint8_t *ptr = (uint8_t *)&n->bar;
+    uint64_t val = 0;
+
+    if (unlikely(addr & (sizeof(uint32_t) - 1))) {
+        error_report("MMIO read not 32-bit aligned, offset=0x%"PRIx64"", addr);
+        // should RAZ, fall through for now
+    } else if (unlikely(size < sizeof(uint32_t))) {
+        error_report("MMIO read smaller than 32-bits,"
+                     " offset=0x%"PRIx64"", addr);
+        // should RAZ, fall through for now
+    }
+
+    if (addr < sizeof(n->bar)) {
+        memcpy(&val, ptr + addr, size);
+    } else {
+        error_report("MMIO read beyond last register,"
+                   " offset=0x%"PRIx64", returning 0", addr);
+    }
+    return val;
+}
+
+static void nvme_process_admin_cmd(NvmeSQueue *sq)
+{
+    NvmeCtrl *n = sq->ctrl;
+    NvmeCQueue *cq = n->cq[sq->cqid];
+    uint16_t status;
+    hwaddr addr;
+    NvmeCmd cmd;
+    NvmeCqe cqe;
+
+    while (!(nvme_sq_empty(sq))) {
+        addr = sq->dma_addr + sq->head * n->sqe_size;
+        pci_dma_read(&n->parent_obj, addr, (void *)&cmd, sizeof(cmd));
+        nvme_inc_sq_head(sq);
+
+        memset(&cqe, 0, sizeof(cqe));
+
+        status = nvme_admin_cmd(n, &cmd, &cqe);
+        cqe.cid = cmd.cid;
+        cqe.status = cpu_to_le16(status << 1 | cq->phase);
+        cqe.sq_id = cpu_to_le16(sq->sqid);
+        cqe.sq_head = cpu_to_le16(sq->head);
+        addr = cq->dma_addr + cq->tail * n->cqe_size;
+        nvme_inc_cq_tail(cq);
+        pci_dma_write(&n->parent_obj, addr, &cqe, sizeof(cqe));
+        nvme_isr_notify(n, cq);
+    }
+}
+
+static void nvme_process_admin_db(NvmeCtrl *n, hwaddr addr, int val)
+{
+    if (((addr - 0x1000) >> 2) & 1) {
+        uint16_t new_head = val & 0xffff;
+        NvmeCQueue *cq;
+        cq = n->cq[0];
+        if (new_head >= cq->size) {
+            return;
+        }
+
+        cq->head = new_head;
+
+        if (cq->tail != cq->head) {
+            nvme_isr_notify(n, cq);
+        }
+    } else {
+        uint16_t new_tail = val & 0xffff;
+        NvmeSQueue *sq;
+        sq = n->sq[0];
+        if (new_tail >= sq->size) {
+            return;
+        }
+
+        sq->tail = new_tail;
+        nvme_process_admin_cmd(sq);
+    }
+}
+
+static void
+nvme_process_io_db(NvmeCtrl *n, hwaddr addr, int val)
+{
+    uint16_t cq_head, sq_tail;
+    uint32_t qid;
+
+    /* Do nothing after the doorbell buffer config command */
+    if (n->dataplane_started) {
+        return;
+    }
+
+    if (((addr - 0x1000) >> 2) & 1) {
+        qid = (addr - (0x1000 + (1 << 2))) >> 3;
+        cq_head = val & 0xffff;
+        if (n->barmem) {
+            n->shadow_db[qid * 2 + 1] = cq_head;
+        }
+    } else {
+        qid = (addr - 0x1000) >> 3;
+        sq_tail = val & 0xffff;
+        if (n->barmem) {
+            n->shadow_db[qid * 2] = sq_tail;
+        }
+    }
+
+    return;
+}
+
+static void nvme_mmio_write(void *opaque, hwaddr addr, uint64_t data,
+    unsigned size)
+{
+    NvmeCtrl *n = (NvmeCtrl *)opaque;
+    if (addr < sizeof(n->bar)) {
+        nvme_write_bar(n, addr, data, size);
+    } else if (addr >= 0x1000 && addr < 0x1008) {
+        nvme_process_admin_db(n, addr, data);
+    } else {
+        nvme_process_io_db(n, addr, data);
+    }
+}
+
+static const MemoryRegionOps nvme_mmio_ops = {
+    .read = nvme_mmio_read,
+    .write = nvme_mmio_write,
+    .endianness = DEVICE_LITTLE_ENDIAN,
+    .impl = {
+        .min_access_size = 2,
+        .max_access_size = 8,
+    },
+};
+
+static void nvme_cleanup(NvmeCtrl *n)
+{
+    g_free(n->sq);
+    g_free(n->cq);
+    g_free(n->namespaces);
+}
+
+static void nvme_realize(PCIDevice *pci_dev, Error **errp)
+{
+    NvmeCtrl *n = NVME_VHOST(pci_dev);
+    NvmeIdCtrl *id = &n->id_ctrl;
+    NvmeNamespace *ns;
+    NvmeIdentify cmd;
+    int ret, i;
+    uint8_t *pci_conf;
+    uint64_t bar_cap;
+
+    if (!n->chardev.chr) {
+        error_setg(errp, "vhost-user-nvme: missing chardev");
+        return;
+    }
+
+    if (!vhost_user_init(&n->vhost_user, &n->chardev, errp)) {
+        return;
+    }
+
+    if (vhost_dev_nvme_init(&n->dev, &n->vhost_user,
+                         VHOST_BACKEND_TYPE_USER, 0) < 0) {
+        error_setg(errp, "vhost-user-nvme: vhost_dev_init failed");
+        return;
+    }
+
+    n->reg_size = pow2ceil(0x1004 + 2 * (n->num_io_queues + 2) * 4);
+
+    /* nvme_init_state */
+    n->sq = g_new0(NvmeSQueue *, n->num_io_queues + 1);
+    n->cq = g_new0(NvmeCQueue *, n->num_io_queues + 1);
+    assert(n->sq != NULL);
+    assert(n->cq != NULL);
+
+    /* nvme_init_pci */
+    pci_conf = pci_dev->config;
+    pci_conf[PCI_INTERRUPT_PIN] = 1;
+    pci_config_set_prog_interface(pci_dev->config, 0x2);
+    pci_config_set_class(pci_dev->config, PCI_CLASS_STORAGE_EXPRESS);
+    pcie_endpoint_cap_init(&n->parent_obj, 0x80);
+
+    memory_region_init_io(&n->iomem, OBJECT(n), &nvme_mmio_ops, n,
+                          "nvme", n->reg_size);
+
+    pci_register_bar(&n->parent_obj, 0,
+        PCI_BASE_ADDRESS_SPACE_MEMORY | PCI_BASE_ADDRESS_MEM_TYPE_64,
+        &n->iomem);
+    msix_init_exclusive_bar(&n->parent_obj, n->num_io_queues, 4, NULL);
+
+    /* Create shadow BAR0 for NVMe controller */
+    if (n->barmem) {
+        n->shadow_mr = host_memory_backend_get_memory(n->barmem);
+        ret = vhost_user_nvme_set_bar_mr(&n->dev, n->shadow_mr);
+        if (ret < 0) {
+            error_report("vhost-user-nvme: set shadow BAR0 MR failed");
+            return;
+        }
+        n->shadow_db = (volatile uint32_t *)((uintptr_t)
+                       memory_region_get_ram_ptr(n->shadow_mr) + 0x1000ull);
+        info_report("Emulated Controller BAR0 0x%"PRIx64"",
+                   (uintptr_t)n->shadow_db);
+    }
+
+    /* Get PCI capabilities via socket */
+    n->bar.cap = 0;
+    ret = vhost_user_nvme_get_cap(&n->dev, &bar_cap);
+    if (ret < 0) {
+        error_setg(errp, "vhost-user-nvme: get controller capabilities failed");
+        return;
+    }
+    n->bar.cap = bar_cap;
+    info_report("Emulated Controller Capabilities 0x%"PRIx64"", n->bar.cap);
+
+    /* TODO: Don't support Controller Memory Buffer and AER now */
+    n->bar.vs = 0x00010000;
+    n->bar.intmc = n->bar.intms = 0;
+
+    /* Get Identify Controller/NS from slave
+     * target and cache the data in QEMU.
+     */
+    cmd.opcode = NVME_ADM_CMD_IDENTIFY;
+    cmd.cns = 0x1;
+    ret = vhost_user_nvme_admin_cmd_raw(&n->dev, (NvmeCmd *)&cmd,
+                                        id, sizeof(*id));
+    if (ret < 0) {
+        error_setg(errp, "vhost-user-nvme: get identify controller failed");
+        return;
+    }
+
+    /* setup a namespace if the controller drive property was given */
+    if (n->namespace.blkconf.blk) {
+        ns = &n->namespace;
+        ns->params.nsid = 1;
+
+        if (nvme_ns_setup(n, ns, errp)) {
+            return;
+        }
+    }
+
+    for (i = 1; i <= id->nn; i++) {
+        cmd.opcode = NVME_ADM_CMD_IDENTIFY;
+        cmd.cns = 0x0;
+        cmd.nsid = i;
+        // with &n->namespace or &n->namespaces ??? maybe need fix
+        ret = vhost_user_nvme_admin_cmd_raw(&n->dev, (NvmeCmd *)&cmd,
+                                            &n->namespaces[i - 1],
+                                            sizeof(NvmeNamespace));
+        if (ret < 0) {
+            error_setg(errp, "vhost-user-nvme: get ns %d failed", i);
+            goto err;
+        }
+    }
+
+    return;
+
+err:
+    nvme_cleanup(n);
+}
+
+static void nvme_exit(PCIDevice *pci_dev)
+{
+    NvmeCtrl *n = NVME_VHOST(pci_dev);
+
+    nvme_cleanup(n);
+    msix_uninit_exclusive_bar(pci_dev);
+}
+
+static Property nvme_props[] = {
+    DEFINE_PROP_UINT32("num_io_queues", NvmeCtrl, num_io_queues, 1),
+    DEFINE_PROP_LINK("barmem", NvmeCtrl, barmem, TYPE_MEMORY_BACKEND, HostMemoryBackend *),
+    DEFINE_PROP_CHR("chardev", NvmeCtrl, chardev),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static const VMStateDescription nvme_vmstate = {
+    .name = "nvme",
+    .unmigratable = 1,
+};
+
+static void nvme_class_init(ObjectClass *oc, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(oc);
+    PCIDeviceClass *pc = PCI_DEVICE_CLASS(oc);
+
+    pc->realize = nvme_realize;
+    pc->exit = nvme_exit;
+    pc->class_id = PCI_CLASS_STORAGE_EXPRESS;
+    pc->vendor_id = PCI_VENDOR_ID_INTEL;
+    pc->device_id = 0x5845;
+    pc->revision = 2;
+
+    set_bit(DEVICE_CATEGORY_STORAGE, dc->categories);
+    dc->desc = "Non-Volatile Memory Express";
+    device_class_set_props(dc, nvme_props);
+    dc->vmsd = &nvme_vmstate;
+}
+
+static void nvme_instance_init(Object *obj)
+{
+    NvmeCtrl *s = NVME_VHOST(obj);
+
+    device_add_bootindex_property(obj, &s->bootindex,
+                                  "bootindex", "/namespace@1,0",
+                                  DEVICE(obj));
+}
+
+static const TypeInfo nvme_info = {
+    .name          = "vhost-user-nvme",
+    .parent        = TYPE_PCI_DEVICE,
+    .instance_size = sizeof(NvmeCtrl),
+    .class_init    = nvme_class_init,
+    .instance_init = nvme_instance_init,
+    .interfaces = (InterfaceInfo[]) {
+        { INTERFACE_PCIE_DEVICE },
+        { }
+    },
+};
+
+static void nvme_register_types(void)
+{
+    type_register_static(&nvme_info);
+}
+
+type_init(nvme_register_types)
diff --git a/hw/block/vhost_user_nvme.h b/hw/block/vhost_user_nvme.h
new file mode 100644
index 00000000000..8a63f4a006b
--- /dev/null
+++ b/hw/block/vhost_user_nvme.h
@@ -0,0 +1,35 @@
+#ifndef HW_VHOST_USER_NVME_H
+#define HW_VHOST_USER_NVME_H
+/*
+ * vhost-user-nvme
+ *
+ * Copyright (c) 2019 Intel Corporation. All rights reserved.
+ *
+ *  Author:
+ *  Changpeng Liu <changpeng.liu@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2 or later.
+ * See the COPYING file in the top-level directory.
+ *
+ */
+
+#include "hw/pci/pci.h"
+#include "hw/block/block.h"
+#include "nvme.h"
+
+int vhost_dev_nvme_set_guest_notifier(struct vhost_dev *hdev,
+                                      EventNotifier *notifier, uint32_t qid);
+int vhost_dev_nvme_init(struct vhost_dev *hdev, void *opaque,
+                   VhostBackendType backend_type, uint32_t busyloop_timeout);
+void vhost_dev_nvme_cleanup(struct vhost_dev *hdev);
+
+int vhost_user_nvme_admin_cmd_raw(struct vhost_dev *dev, NvmeCmd *cmd,
+                                  void *buf, uint32_t len);
+int vhost_user_nvme_get_cap(struct vhost_dev *dev, uint64_t *cap);
+int vhost_user_nvme_set_bar_mr(struct vhost_dev *dev, MemoryRegion *mr);
+int vhost_dev_nvme_set_backend_type(struct vhost_dev *dev,
+                                    VhostBackendType backend_type);
+int vhost_dev_nvme_start(struct vhost_dev *hdev, VirtIODevice *vdev);
+int vhost_dev_nvme_stop(struct vhost_dev *hdev);
+
+#endif
diff --git a/hw/virtio/vhost-user.c b/hw/virtio/vhost-user.c
index 2fdd5daf74b..f706ced336c 100644
--- a/hw/virtio/vhost-user.c
+++ b/hw/virtio/vhost-user.c
@@ -25,6 +25,10 @@
 #include "migration/postcopy-ram.h"
 #include "trace.h"

+#include "hw/block/block.h"
+#include "include/block/nvme.h"
+#include "hw/block/vhost_user_nvme.h"
+
 #include <sys/ioctl.h>
 #include <sys/socket.h>
 #include <sys/un.h>
@@ -124,6 +128,11 @@ typedef enum VhostUserRequest {
     VHOST_USER_GET_MAX_MEM_SLOTS = 36,
     VHOST_USER_ADD_MEM_REG = 37,
     VHOST_USER_REM_MEM_REG = 38,
+    VHOST_USER_NVME_ADMIN = 80,
+    VHOST_USER_NVME_SET_CQ_CALL = 81,
+    VHOST_USER_NVME_GET_CAP = 82,
+    VHOST_USER_NVME_START_STOP = 83,
+    VHOST_USER_NVME_SET_BAR_MR = 85,
     VHOST_USER_MAX
 } VhostUserRequest;

@@ -213,6 +222,13 @@ typedef union {
         VhostUserMemory memory;
         VhostUserMemRegMsg mem_reg;
         VhostUserLog log;
+        struct nvme {
+            union {
+                NvmeCmd req;
+                NvmeCqe cqe;
+            } cmd;
+            uint8_t buf[4096];
+        } nvme;
         struct vhost_iotlb_msg iotlb;
         VhostUserConfig config;
         VhostUserCryptoSession session;
@@ -2387,3 +2403,188 @@ const VhostOps user_ops = {
         .vhost_get_inflight_fd = vhost_user_get_inflight_fd,
         .vhost_set_inflight_fd = vhost_user_set_inflight_fd,
 };
+
+int vhost_user_nvme_get_cap(struct vhost_dev *dev, uint64_t *cap)
+{
+    return vhost_user_get_u64(dev, VHOST_USER_NVME_GET_CAP, cap);
+}
+
+int vhost_dev_nvme_start(struct vhost_dev *dev, VirtIODevice *vdev)
+{
+    int r;
+
+    if (vdev != NULL) {
+        return -1;
+    }
+    r = dev->vhost_ops->vhost_set_mem_table(dev, dev->mem);
+    if (r < 0) {
+        error_report("SET MEMTABLE Failed");
+        return -1;
+    }
+
+    vhost_user_set_u64(dev, VHOST_USER_NVME_START_STOP, 1);
+
+    return 0;
+}
+
+int vhost_dev_nvme_stop(struct vhost_dev *dev)
+{
+    return vhost_user_set_u64(dev, VHOST_USER_NVME_START_STOP, 0);
+}
+
+int vhost_user_nvme_set_bar_mr(struct vhost_dev *dev, MemoryRegion *mr)
+{
+    int fds[1];
+    bool reply_supported = virtio_has_feature(dev->protocol_features,
+                                          VHOST_USER_PROTOCOL_F_REPLY_ACK);
+
+    VhostUserMsg msg = {
+        .hdr.request = VHOST_USER_NVME_SET_BAR_MR,
+        .hdr.flags = VHOST_USER_VERSION,
+    };
+
+    if (reply_supported) {
+        msg.hdr.flags |= VHOST_USER_NEED_REPLY_MASK;
+    }
+
+    msg.payload.memory.regions[0].userspace_addr = (uintptr_t)
+                                                 memory_region_get_ram_ptr(mr);
+    msg.payload.memory.regions[0].memory_size  = memory_region_size(mr);
+    msg.payload.memory.regions[0].guest_phys_addr =
+                                                 memory_region_get_ram_addr(mr);
+    msg.payload.memory.regions[0].mmap_offset = 0;
+
+    fds[0] = memory_region_get_fd(mr);
+    if (fds[0] < 0) {
+        error_report("error controller BAR memory region");
+        return -1;
+    }
+
+    msg.payload.memory.nregions = 1;
+    msg.hdr.size = sizeof(msg.payload.memory.nregions);
+    msg.hdr.size += sizeof(msg.payload.memory.padding);
+    msg.hdr.size += sizeof(VhostUserMemoryRegion);
+
+    if (vhost_user_write(dev, &msg, fds, 1) < 0) {
+        return -1;
+    }
+
+    if (reply_supported) {
+        return process_message_reply(dev, &msg);
+    }
+
+    return 0;
+}
+
+/* reply required for all the messages */
+int vhost_user_nvme_admin_cmd_raw(struct vhost_dev *dev, NvmeCmd *cmd,
+                                  void *buf, uint32_t len)
+{
+    VhostUserMsg msg = {
+        .hdr.request = VHOST_USER_NVME_ADMIN,
+        .hdr.flags = VHOST_USER_VERSION,
+    };
+    uint16_t status;
+
+    msg.hdr.size = sizeof(*cmd);
+    memcpy(&msg.payload.nvme.cmd.req, cmd, sizeof(*cmd));
+
+    if (vhost_user_write(dev, &msg, NULL, 0) < 0) {
+        return -1;
+    }
+
+    if (vhost_user_read(dev, &msg) < 0) {
+        return -1;
+    }
+
+    if (msg.hdr.request != VHOST_USER_NVME_ADMIN) {
+        error_report("Received unexpected msg type. Expected %d received %d",
+                     VHOST_USER_NVME_ADMIN, msg.hdr.request);
+        return -1;
+    }
+
+    switch (cmd->opcode) {
+    case NVME_ADM_CMD_DELETE_SQ :
+    case NVME_ADM_CMD_CREATE_SQ :
+    case NVME_ADM_CMD_DELETE_CQ :
+    case NVME_ADM_CMD_CREATE_CQ :
+    case NVME_ADM_CMD_SET_DB_MEMORY :
+    case NVME_ADM_CMD_GET_FEATURES :
+    case NVME_ADM_CMD_SET_FEATURES :
+        if (msg.hdr.size != sizeof(NvmeCqe)) {
+            error_report("Received unexpected rsp message. %u received %u",
+                         cmd->opcode, msg.hdr.size);
+        }
+        status = msg.payload.nvme.cmd.cqe.status;
+        if (nvme_cpl_is_error(status)) {
+            error_report("Nvme Admin Command Status Faild");
+            return -1;
+        }
+        memcpy(buf, &msg.payload.nvme.cmd.cqe, len);
+    break;
+    case NVME_ADM_CMD_IDENTIFY :
+        if (msg.hdr.size != sizeof(NvmeCqe) + 4096) {
+            error_report("Received unexpected rsp message. %u received %u",
+                         cmd->opcode, msg.hdr.size);
+        }
+        status = msg.payload.nvme.cmd.cqe.status;
+        if (nvme_cpl_is_error(status)) {
+            error_report("Nvme Admin Command Status Faild");
+            return -1;
+        }
+        memcpy(buf, &msg.payload.nvme.buf, len);
+    break;
+    default:
+        return -1;
+    }
+
+    return 0;
+}
+
+static int vhost_user_nvme_set_vring_call(struct vhost_dev *dev,
+                                     struct vhost_vring_file *file)
+{
+    return vhost_set_vring_file(dev, VHOST_USER_NVME_SET_CQ_CALL, file);
+}
+
+static int vhost_user_nvme_init(struct vhost_dev *dev, void *opaque)
+{
+    struct vhost_user *u;
+
+    assert(dev->vhost_ops->backend_type == VHOST_BACKEND_TYPE_USER);
+
+    u = g_new0(struct vhost_user, 1);
+    u->user = opaque;
+    u->slave_fd = -1;
+    u->dev = dev;
+    dev->opaque = u;
+
+    return 0;
+}
+
+static const VhostOps user_nvme_ops = {
+        .backend_type = VHOST_BACKEND_TYPE_USER,
+        .vhost_backend_init = vhost_user_nvme_init,
+        .vhost_backend_cleanup = vhost_user_backend_cleanup,
+        .vhost_backend_memslots_limit = vhost_user_memslots_limit,
+        .vhost_set_mem_table = vhost_user_set_mem_table,
+        .vhost_set_vring_call = vhost_user_nvme_set_vring_call,
+        .vhost_backend_can_merge = vhost_user_can_merge,
+};
+
+int vhost_dev_nvme_set_backend_type(struct vhost_dev *dev,
+                                    VhostBackendType backend_type)
+{
+    int r = 0;
+
+    switch (backend_type) {
+    case VHOST_BACKEND_TYPE_USER :
+        dev->vhost_ops = &user_nvme_ops;
+        break;
+    default:
+        error_report("Unknown vhost backend type");
+        r = -1;
+    }
+
+    return r;
+}
diff --git a/hw/virtio/vhost.c b/hw/virtio/vhost.c
index 614ccc2bcb6..764ef9644f1 100644
--- a/hw/virtio/vhost.c
+++ b/hw/virtio/vhost.c
@@ -24,6 +24,7 @@
 #include "exec/address-spaces.h"
 #include "hw/virtio/virtio-bus.h"
 #include "hw/virtio/virtio-access.h"
+#include "hw/block/vhost_user_nvme.h"
 #include "migration/blocker.h"
 #include "migration/qemu-file-types.h"
 #include "sysemu/dma.h"
@@ -1822,3 +1823,114 @@ int vhost_net_set_backend(struct vhost_dev *hdev,

     return -1;
 }
+
+int vhost_dev_nvme_init(struct vhost_dev *hdev, void *opaque,
+                   VhostBackendType backend_type, uint32_t busyloop_timeout)
+{
+    int r;
+    Error *local_err = NULL;
+
+    hdev->vdev = NULL;
+    hdev->migration_blocker = NULL;
+
+    r = vhost_dev_nvme_set_backend_type(hdev, backend_type);
+    assert(r >= 0);
+
+    r = hdev->vhost_ops->vhost_backend_init(hdev, opaque);
+    if (r < 0) {
+        goto fail;
+    }
+
+    hdev->memory_listener = (MemoryListener) {
+        .begin = vhost_begin,
+        .commit = vhost_commit,
+        .region_add = vhost_region_addnop,
+        .region_nop = vhost_region_addnop,
+        .log_start = vhost_log_start,
+        .log_stop = vhost_log_stop,
+        .log_sync = vhost_log_sync,
+        .log_global_start = vhost_log_global_start,
+        .log_global_stop = vhost_log_global_stop,
+        .eventfd_add = vhost_eventfd_add,
+        .eventfd_del = vhost_eventfd_del,
+        .priority = 10
+    };
+
+    hdev->iommu_listener = (MemoryListener) {
+        .region_add = vhost_iommu_region_add,
+        .region_del = vhost_iommu_region_del,
+    };
+
+    if (hdev->migration_blocker == NULL) {
+        if (!(hdev->features & (0x1ULL << VHOST_F_LOG_ALL))) {
+            error_setg(&hdev->migration_blocker,
+                       "Migration disabled: vhost lacks VHOST_F_LOG_ALL feature.");
+        } else if (vhost_dev_log_is_shared(hdev) && !qemu_memfd_alloc_check()) {
+            error_setg(&hdev->migration_blocker,
+                       "Migration disabled: failed to allocate shared memory");
+        }
+    }
+
+    if (hdev->migration_blocker != NULL) {
+        r = migrate_add_blocker(hdev->migration_blocker, &local_err);
+        if (local_err) {
+            error_report_err(local_err);
+            error_free(hdev->migration_blocker);
+            goto fail;
+        }
+    }
+
+    hdev->mem = g_malloc0(offsetof(struct vhost_memory, regions));
+    hdev->n_mem_sections = 0;
+    hdev->mem_sections = NULL;
+    hdev->log = NULL;
+    hdev->log_size = 0;
+    hdev->log_enabled = false;
+    hdev->started = false;
+    memory_listener_register(&hdev->memory_listener, &address_space_memory);
+    QLIST_INSERT_HEAD(&vhost_devices, hdev, entry);
+
+    if (used_memslots > hdev->vhost_ops->vhost_backend_memslots_limit(hdev)) {
+        error_report("vhost backend memory slots limit is less"
+                " than current number of present memory slots");
+        r = -1;
+        goto fail;
+    }
+
+    return 0;
+
+fail:
+    vhost_dev_nvme_cleanup(hdev);
+    return r;
+}
+
+void vhost_dev_nvme_cleanup(struct vhost_dev *hdev)
+{
+    if (hdev->mem) {
+        /* those are only safe after successful init */
+        memory_listener_unregister(&hdev->memory_listener);
+        QLIST_REMOVE(hdev, entry);
+    }
+    if (hdev->migration_blocker) {
+        migrate_del_blocker(hdev->migration_blocker);
+        error_free(hdev->migration_blocker);
+    }
+    g_free(hdev->mem);
+    g_free(hdev->mem_sections);
+    if (hdev->vhost_ops) {
+        hdev->vhost_ops->vhost_backend_cleanup(hdev);
+    }
+    assert(!hdev->log);
+
+    memset(hdev, 0, sizeof(struct vhost_dev));
+}
+
+int vhost_dev_nvme_set_guest_notifier(struct vhost_dev *hdev,
+                                      EventNotifier *notifier, uint32_t qid)
+{
+    struct vhost_vring_file file;
+
+    file.fd = event_notifier_get_fd(notifier);
+    file.index = qid;
+    return hdev->vhost_ops->vhost_set_vring_call(hdev, &file);
+}
diff --git a/include/block/nvme.h b/include/block/nvme.h
index 3e02d9ca984..da3e0d61545 100644
--- a/include/block/nvme.h
+++ b/include/block/nvme.h
@@ -459,6 +459,7 @@ enum NvmeAdminCommands {
     NVME_ADM_CMD_ASYNC_EV_REQ   = 0x0c,
     NVME_ADM_CMD_ACTIVATE_FW    = 0x10,
     NVME_ADM_CMD_DOWNLOAD_FW    = 0x11,
+    NVME_ADM_CMD_SET_DB_MEMORY  = 0x7c,
     NVME_ADM_CMD_FORMAT_NVM     = 0x80,
     NVME_ADM_CMD_SECURITY_SEND  = 0x81,
     NVME_ADM_CMD_SECURITY_RECV  = 0x82,
